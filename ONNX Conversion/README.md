# Convertion Instructions
## ONNX Format
ONNX is our desired ML model format for easier transferrability to other platforms and easier deployment with NVIDIA TensorRT.
To know more about this format, check: https://onnx.ai/ 

## NPZ To PB 
In the Hyperpose docs, it is explained that in order to convert to ONNX, we should first convert to protobuf

In order to convert successfully to protobuf, we need to make some modifications to the tensorlayer code in order to be compatible with the rather special NPZ format generated by the Hyperpose authors. 
The issue with their NPZ format is that it does not include a possibility to call `['params']` and get all the parameters of the model.

- First, go to the directory where your python files are installed by default, then go to `tensorlayer/files/utils.py`
  > _For `Conda` users, python pachages are installed in `/home/user/anaconda3/envs/Environment/lib/python3.7/site-packages/..`_
- Go to `line 1960` and replace the existing ***load_npz*** function with:

```python
def load_npz(path='', name='model.npz'):
    """
    Some useless Notes...
    
    """
    d = np.load(os.path.join(path, name), allow_pickle=True)
    return d#['params']
```

- Now go ***line 1991*** and replace the existing ***assign_weights*** function with:

```python
def assign_weights(d, network):
    """
    Some useless Notes...
    
    """
    ops = []
    for i,j in enumerate(network.all_weights):
        print(str(i) + ' for Debuggging by Elio')
        ops.append(network.all_weights[i].assign(d[network.all_weights[i].name]))
    return ops
```

>**That's all you need to change for now!**

### Conversion to PB
- In order to convert to PB format, we will use `export_pb.py` found in the hyperpose rpository.
- we then run the following terminal command:
- 
```bash
# From the hyperpose directory:
python3 export_pb.py --model_name=${Your_Model} --model_type=LightweightOpenpose --model_backbone=Vggtiny
```



