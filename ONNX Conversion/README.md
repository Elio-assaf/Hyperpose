# Convertion Instructions
## ONNX Format
ONNX is our desired ML model format for easier transferrability to other platforms and easier deployment with NVIDIA TensorRT.
To know more about this format, check: https://onnx.ai/ 

## NPZ To PB 
In the Hyperpose docs, it is explained that in order to convert to ONNX, we should first convert to protobuf

In order to convert successfully to protobuf, we need to make some modifications to the tensorlayer code in order to be compatible with the rather special NPZ format generated by the Hyperpose authors. 
The issue with their NPZ format is that it does not include a possibility to call `['params']` and get all the parameters of the model.

- First, go to the directory where your python files are installed by default, then go to `tensorlayer/files/utils.py`
  > _For `Conda` users, python pachages are installed in `/home/user/anaconda3/envs/Environment/lib/python3.7/site-packages/..`_
- Go to `line 1960` and replace the existing ***load_npz*** function with:

```python
def load_npz(path='', name='model.npz'):
    """
    Some useless Notes...
    
    """
    d = np.load(os.path.join(path, name), allow_pickle=True)
    return d#['params']
```

- Now go ***line 1991*** and replace the existing ***assign_weights*** function with:

```python
def assign_weights(d, network):
    """
    Some useless Notes...
    
    """
    ops = []
    for i,j in enumerate(network.all_weights):
        ops.append(network.all_weights[i].assign(d[network.all_weights[i].name]))
    return ops
```

>**That's all you need to change for now!**

### Conversion to PB
- In order to convert to PB format, we will use `export_pb.py` found in the hyperpose rpository.
- we then run the following terminal command:

```bash
# From the hyperpose directory:
python3 export_pb.py --model_name=${Your_Model} --model_type=LightweightOpenpose --model_backbone=Vggtiny
```
> **Here you go, we're already half way through the process**
For the Hyperpose version of this convertion, check: 
## PB to ONNX

After the Conversion to PB, the terminal output should look similar to the following
```bash
Skipping registering GPU devices...
2022-06-24 16:53:42.238658: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-06-24 16:53:42.238663: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 
2022-06-24 16:53:42.238667: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N 
2022-06-24 16:53:42.269458: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: graph_to_optimize
2022-06-24 16:53:42.269470: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: Graph size after: 1168 nodes (996), 1220 edges (1047), time = 13.189ms.
2022-06-24 16:53:42.269475: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.654ms.
Exporting pb file finished! output file: save_dir/Test/frozen_Test.pb
Exported graph INPUT nodes: ['x']
Exported graph OUTPUT nodes: ['Identity_1', 'Identity']
```
- `In order to convert to ONNX, use the sollowing terminal commandS: `
```bash
# Installing tf2onnx package
pip install tf2onnx
# Converting our .pb file with it:
python3 -m tf2onnx.convert --graphdef frozen_MyProtobufFile.pb --output Testing_the_ONNX_convertion.onnx --inputs x:0 --outputs Identity_1:0,Identity:0

# You can also do it by first exporting the variables one by one and doing the following:

export INPUT_NODE_NAME=x
export INPUT_NODE_NAME0=Identity
export INPUT_NODE_NAME1=Identity_1
export OUTPUT_ONNX_MODEL=my_output_model.onnx

python3 -m tf2onnx.convert --graphdef frozen_${MODEL_NAME}.pb --output ${OUTPUT_ONNX_MODEL} --inputs ${INPUT_NODE_NAME}:0 --outputs ${INPUT_NODE_NAME0}:0,${INPUT_NODE_NAME1}:0

```
> **`Great Work!, now you can use your ONNX model for deployment`**


